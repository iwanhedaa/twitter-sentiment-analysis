import streamlit as st
import re
import numpy as np
import tensorflow as tf
import pickle
from parsivar import Normalizer as ParsivarNormalizer
from parsivar import Tokenizer as ParsivarTokenizer

# Load model and tokenizer
model = tf.keras.models.load_model("saved_lstm_model_with_callbacks.h5")
with open("tokenizer.pkl", "rb") as handle:
    tokenizer = pickle.load(handle)


# Emoji and keyword mappings
emoji_to_persian = {
    'ðŸ˜‚': ' Ø®Ù†Ø¯Ù‡ ', 'ðŸ¤£': ' Ø®Ù†Ø¯Ù‡ ', 'ðŸ˜Š': ' Ù„Ø¨Ø®Ù†Ø¯ ', 'ðŸ˜': ' Ø¹Ø´Ù‚ ', 'â¤ï¸': ' Ù‚Ù„Ø¨ ', 'ðŸ’•': ' Ø¹Ø´Ù‚ ',
    'ðŸ‘': ' Ù„Ø§ÛŒÚ© ', 'ðŸ”¥': ' Ø¢ØªØ´ ', 'ðŸ™': ' ØªØ´Ú©Ø± ', 'ðŸŒ¸': ' Ú¯Ù„ ', 'ðŸŽ‰': ' Ø¬Ø´Ù† ',
    'ðŸ’”': ' Ù‚Ù„Ø¨_Ø´Ú©Ø³ØªÙ‡ ', 'ðŸ˜”': ' ØºÙ…Ú¯ÛŒÙ† ', 'ðŸ˜ž': ' Ù†Ø§Ø§Ù…ÛŒØ¯ ', 'ðŸ™': ' Ù†Ø§Ø±Ø§Ø­Øª ',
    'ðŸ¤”': ' ØªÙÚ©Ø± ', 'ðŸ‘Ž': ' Ø¯ÛŒØ³Ù„Ø§ÛŒÚ© ', 'ðŸ˜ ': ' Ø¹ØµØ¨Ø§Ù†ÛŒ ', 'ðŸ˜¡': ' Ø¹ØµØ¨Ø§Ù†ÛŒ ', 'âœ…': ' ØªÛŒÚ©_Ø³Ø¨Ø² ',
    'âŒ': ' Ø¶Ø±Ø¨Ø¯Ø± ', 'ðŸŒ¹': ' Ú¯Ù„ ',
    'ðŸ˜­': ' Ú¯Ø±ÛŒÙ‡ '
}

positive_keywords_for_cry_emoji = [
    "Ø²ÛŒØ¨Ø§ÛŒÛŒ", "Ø²ÛŒØ¨Ø§", "Ø®ÙˆØ´Ø¨Ø®Øª", "Ù†Ø§Ø²", "Ø®Ø´Ú©Ù„", "Ø®ÙˆØ´", "Ø®Ø±", "Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ", "Ø¬Ø´Ù†", "Ø´Ø§Ø¯ÛŒ", "Ù‚Ø´Ù†Ú¯",
    "Ø´ÙˆÙ‚", "Ø®ÙˆØ´Ø­Ø§Ù„Ù…", "Ù‡ÛŒØ¬Ø§Ù†", "Ø°ÙˆÙ‚", "Ø´Ø§Ø¯", "Ù…Ø¨Ø§Ø±Ú©", "ØªØ¨Ø±ÛŒÚ©",
    "Ø¹Ø§Ù„ÛŒ", "ÙÙˆÙ‚_Ø§Ù„Ø¹Ø§Ø¯Ù‡", "Ù…Ø­Ø´Ø±", "Ø³ÙˆØ±Ù¾Ø±Ø§ÛŒØ²_Ø®ÙˆØ¨", "Ø§ÙØªØ®Ø§Ø±"
]
negative_keywords_for_cry_emoji = [
    "ØºÙ…", "Ù†Ø§Ø±Ø§Ø­Øª", "Ø§Ù†Ø¯ÙˆÙ‡", "Ø¯Ù„ØªÙ†Ú¯", "Ø§ÙØ³ÙˆØ³", "Ù…ØªØ§Ø³ÙÙ…", "ØªØ³Ù„ÛŒØª", "Ø¯Ø±Ø¯", "Ø¯Ø§Øº",
    "Ø´Ú©Ø³Øª", "Ø¨Ø§Ø®Øª", "Ù…ØµÛŒØ¨Øª", "ÙØ§Ø¬Ø¹Ù‡", "Ø¨Ø¯Ø¨Ø®Øª", "Ù…Ø±Ú¯", "ÙÙˆØª", "Ú¯Ø±ÛŒÙ‡_Ú©Ù†Ø§Ù†", "Ù†Ø§Ø±Ø§Ø¬ØªÛŒ"
]

contractions_dict = {
    "Ù†Ù…ÛŒØ¯ÙˆÙ†Ù…": "Ù†Ù…ÛŒ Ø¯Ø§Ù†Ù…", "Ù†Ù…ÛŒØ®ÙˆØ§Ù…": "Ù†Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù…", "Ù…ÛŒØ¯ÙˆÙ†Ù…": "Ù…ÛŒ Ø¯Ø§Ù†Ù…",
    "Ù†Ù…ÛŒØªÙˆÙ†Ù…": "Ù†Ù…ÛŒ ØªÙˆØ§Ù†Ù…", "Ù…ÛŒØªÙˆÙ†Ù…": "Ù…ÛŒ ØªÙˆØ§Ù†Ù…", "Ù…ÛŒØ´Ù‡": "Ù…ÛŒ Ø´ÙˆØ¯",
    "Ù†Ù…ÛŒØ´Ù‡": "Ù†Ù…ÛŒ Ø´ÙˆØ¯", "Ø®ÙˆØ¨Ù‡": "Ø®ÙˆØ¨ Ø§Ø³Øª", "Ø¨Ø±ÛŒÙ…": "Ø¨Ø±ÙˆÛŒÙ…",
    "Ø§ÛŒÙ†Ø¬ÙˆØ±ÛŒ": "Ø§ÛŒÙ† Ø¬ÙˆØ±ÛŒ", "Ø§ÛŒÙ†Ø§": "Ø§ÛŒÙ† Ù‡Ø§", "Ú†ÛŒÙ‡": "Ú†ÛŒ Ø§Ø³Øª", "Ú©ÛŒÙ‡": "Ú©ÛŒ Ø§Ø³Øª",
    "Ù…ÛŒØ±Ù…": "Ù…ÛŒ Ø±ÙˆÙ…", "Ù†Ù…ÛŒØ§Ù…": "Ù†Ù…ÛŒ Ø¢ÛŒÙ…", "Ù…ÛŒØ§Ù…": "Ù…ÛŒ Ø¢ÛŒÙ…",
    "Ø­Ø§Ù„Ù… Ø®ÙˆØ¨Ù‡": "Ø­Ø§Ù„ Ù…Ù† Ø®ÙˆØ¨ Ø§Ø³Øª", "Ø­Ø§Ù„ Ø®ÙˆØ¨ÛŒ Ù†Ø¯Ø§Ø±Ù…": "Ø­Ø§Ù„ Ø®ÙˆØ¨ÛŒ Ù†Ø¯Ø§Ø±Ù…",
    "ÛŒÙ‡": "ÛŒÚ©"
}

normalizer = ParsivarNormalizer()
tokenizer_p = ParsivarTokenizer()

# Preprocessing function
def preprocess_persian_tweet(text):
    if not isinstance(text, str):
        return ""

    for contraction, expansion in contractions_dict.items():
        text = text.replace(contraction, expansion)

    num_cry_emojis = text.count("ðŸ˜­")
    if num_cry_emojis > 0:
        has_positive = any(k in text for k in positive_keywords_for_cry_emoji)
        has_negative = any(k in text for k in negative_keywords_for_cry_emoji)

        replacement = " Ú¯Ø±ÛŒÙ‡ "
        if has_positive and not has_negative:
            replacement = " Ø§Ø´Ú©_Ø´ÙˆÙ‚ "
        elif has_negative and not has_positive:
            replacement = " Ú¯Ø±ÛŒÙ‡_Ø´Ø¯ÛŒØ¯ "
        text = text.replace("ðŸ˜­", replacement)

    for emoji_char, persian_keyword in emoji_to_persian.items():
        text = text.replace(emoji_char, persian_keyword)

    text = text.replace("_", " ")
    text = normalizer.normalize(text)
    text = re.sub(r"(.)\1{3,}", r"\1\1\1", text)
    text = re.sub(r"@[^\s]+", "", text)
    text = re.sub(r"#([^\s]+)", r"\1", text)
    text = re.sub(r"http\S+", "", text)
    text = re.sub(
        r"[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\U0001EE00-\U0001EEFF\u200C\s\d_ØŸ!?]",
        "", text, flags=re.UNICODE
    )

    text_to_token = text.replace("_", " ")
    tokens = tokenizer_p.tokenize_words(text_to_token)
    tokens = [t.strip() for t in tokens if len(t.strip()) >= 1]
    return " ".join(tokens)

# Streamlit UI
st.set_page_config(page_title="Persian Emotion Classifier", layout="centered")
st.title("ðŸ’¬ Persian Tweet Emotion Classifier")
st.markdown("Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÛŒÚ© ØªÙˆÛŒÛŒØª ÙØ§Ø±Ø³ÛŒ Ø±Ø§ (Ø¨ÛŒÙ† Ø´Ø§Ø¯ÛŒ Ùˆ ØºÙ…) ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.")

user_input = st.text_area("ÛŒÚ© ØªÙˆÛŒÛŒØª ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")

if st.button("Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³"):
    if not user_input.strip():
        st.warning("Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    else:
        preprocessed = preprocess_persian_tweet(user_input)
        sequence = tokenizer.texts_to_sequences([preprocessed])
        padded = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=50)

        prediction = model.predict(padded)[0][0]
        label = "ðŸ˜¢ ØºÙ…" if prediction >= 0.5 else "ðŸ˜Š Ø´Ø§Ø¯ÛŒ"
        st.success(f"Ø§Ø­Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡: **{label}**")
        st.write(f"ðŸ“Š Ø§Ø­ØªÙ…Ø§Ù„ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„: `{prediction:.4f}`")
